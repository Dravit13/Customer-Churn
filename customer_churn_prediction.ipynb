{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Churn Prediction - Machine Learning Model\n",
        "\n",
        "## Project Overview\n",
        "This notebook implements a complete Machine Learning pipeline to predict customer churn for an e-commerce business. The model identifies customers who are likely to discontinue using the company's services, enabling proactive retention strategies.\n",
        "\n",
        "## Objectives\n",
        "- Build a Machine Learning Prediction model to predict Customer Churn\n",
        "- Handle imbalanced datasets using SMOTE\n",
        "- Evaluate models using appropriate metrics\n",
        "- Generate Confusion Matrix and ROC Curve visualizations\n",
        "- Explain evaluation metrics and visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
        ")\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTETomek\n",
        "import kagglehub\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"✓ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Download Dataset\n",
        "\n",
        "Download the dataset from Kaggle using `kagglehub`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download dataset from Kaggle\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 1: Downloading Dataset\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    # Download latest version\n",
        "    path = kagglehub.dataset_download(\"ankitverma2010/ecommerce-customer-churn-analysis-and-prediction\")\n",
        "    print(f\"✓ Dataset downloaded successfully!\")\n",
        "    print(f\"Path to dataset files: {path}\")\n",
        "    \n",
        "    # Find data files (CSV or Excel) in the downloaded directory\n",
        "    data_files = []\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            if file.endswith(('.csv', '.xlsx', '.xls')):\n",
        "                data_files.append(os.path.join(root, file))\n",
        "    \n",
        "    if data_files:\n",
        "        print(f\"\\nFound data files:\")\n",
        "        for data_file in data_files:\n",
        "            print(f\"  - {data_file}\")\n",
        "        file_path = data_files[0]  # Use first data file\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No CSV or Excel files found in downloaded dataset\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Error downloading dataset: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load and Explore Data\n",
        "\n",
        "Load the dataset and explore its structure, missing values, and basic statistics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data based on file extension\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 2: Loading and Exploring Data\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if file_path.endswith('.csv'):\n",
        "    df = pd.read_csv(file_path)\n",
        "elif file_path.endswith(('.xlsx', '.xls')):\n",
        "    # Try to find the data sheet (skip metadata sheets)\n",
        "    xl_file = pd.ExcelFile(file_path)\n",
        "    sheet_names = xl_file.sheet_names\n",
        "    \n",
        "    # Look for common data sheet names or use the largest sheet\n",
        "    data_sheet = None\n",
        "    for sheet in sheet_names:\n",
        "        sheet_lower = sheet.lower()\n",
        "        # Skip metadata/dictionary sheets\n",
        "        if 'dict' not in sheet_lower and 'meta' not in sheet_lower and 'info' not in sheet_lower:\n",
        "            # Check if this sheet has substantial data\n",
        "            test_df = pd.read_excel(file_path, sheet_name=sheet, nrows=5)\n",
        "            if len(test_df.columns) > 2:  # Has multiple columns (likely data)\n",
        "                data_sheet = sheet\n",
        "                break\n",
        "    \n",
        "    # If no suitable sheet found, try the largest sheet\n",
        "    if data_sheet is None:\n",
        "        max_rows = 0\n",
        "        for sheet in sheet_names:\n",
        "            test_df = pd.read_excel(file_path, sheet_name=sheet)\n",
        "            if len(test_df) > max_rows:\n",
        "                max_rows = len(test_df)\n",
        "                data_sheet = sheet\n",
        "    \n",
        "    # Load the data sheet\n",
        "    if data_sheet:\n",
        "        print(f\"\\nLoading data from sheet: '{data_sheet}'\")\n",
        "        df = pd.read_excel(file_path, sheet_name=data_sheet)\n",
        "    else:\n",
        "        # Fallback to first sheet\n",
        "        print(f\"\\nWarning: Could not determine data sheet, using first sheet: '{sheet_names[0]}'\")\n",
        "        df = pd.read_excel(file_path, sheet_name=sheet_names[0])\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported file format: {file_path}\")\n",
        "\n",
        "print(f\"\\n✓ Data loaded successfully!\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset information\n",
        "print(\"Dataset Info:\")\n",
        "df.info()\n",
        "print(\"\\nMissing values:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\nStatistical Summary:\")\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
